# AI Provider Interface

## üìã –û–ø–∏—Å

–ë–∞–∑–æ–≤–∞ –∞–±—Å—Ç—Ä–∞–∫—Ü—ñ—è –¥–ª—è –º—É–ª—å—Ç–∏-AI –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏, —è–∫–∞ –∑–∞–±–µ–∑–ø–µ—á—É—î —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ —Ä—ñ–∑–Ω–∏–º–∏ AI-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ (OpenAI, Google Gemini, Anthropic Claude, —Ç–æ—â–æ).

## üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞

### AIResponse

Pydantic-–º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤—ñ–¥ AI-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤.

**–ü–æ–ª—è:**
- `text` (str, required) ‚Äî –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç
- `provider` (str, required) ‚Äî –Ü–º'—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (`openai`, `gemini`, `claude`)
- `model` (str, required) ‚Äî –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å (`gpt-4o-mini`, `gemini-1.5-flash`)
- `raw` (dict, optional) ‚Äî –°–∏—Ä–∏–π –æ–±'—î–∫—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ (–¥–ª—è debugging)
- `prompt_tokens` (int, optional) ‚Äî –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ —É prompt
- `completion_tokens` (int, optional) ‚Äî –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
- `total_tokens` (int, optional) ‚Äî –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤

**–ü—Ä–∏–∫–ª–∞–¥:**
```python
from src.core.ai import AIResponse

response = AIResponse(
    text="This is AI-generated content",
    provider="openai",
    model="gpt-4o-mini",
    prompt_tokens=50,
    completion_tokens=100,
    total_tokens=150
)
```

### AIProvider

Protocol (—ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å) –¥–ª—è –≤—Å—ñ—Ö AI-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤.

**–ú–µ—Ç–æ–¥–∏:**
- `async generate(prompt: str, **kwargs) -> AIResponse` ‚Äî –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É
- `async embed(text: str, **kwargs) -> list[float]` ‚Äî –°—Ç–≤–æ—Ä–µ–Ω–Ω—è embeddings
- `async count_tokens(text: str, **kwargs) -> int` ‚Äî –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ç–æ–∫–µ–Ω—ñ–≤

**–ü—Ä–∏–∫–ª–∞–¥ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó:**
```python
from src.core.ai import AIProvider, AIResponse

class MyCustomProvider:
    name = "custom"
    
    async def generate(self, prompt: str, **kwargs) -> AIResponse:
        # –í–∞—à–∞ –ª–æ–≥—ñ–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        return AIResponse(
            text="Generated response",
            provider=self.name,
            model="custom-model-v1"
        )
    
    async def embed(self, text: str, **kwargs) -> list[float]:
        # –í–∞—à–∞ –ª–æ–≥—ñ–∫–∞ embeddings
        return [0.1, 0.2, 0.3, ...]
    
    async def count_tokens(self, text: str, **kwargs) -> int:
        # –í–∞—à–∞ –ª–æ–≥—ñ–∫–∞ –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É —Ç–æ–∫–µ–Ω—ñ–≤
        return len(text.split())
```

## üß™ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è

–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç—ñ–≤:
```bash
pytest tests/core/ai/test_interface.py -v
```

–í—Å—ñ —Ç–µ—Å—Ç–∏ –ø–æ–≤–∏–Ω–Ω—ñ –ø—Ä–æ–π—Ç–∏ —É—Å–ø—ñ—à–Ω–æ:
- ‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è AIResponse –∑ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–º–∏ –ø–æ–ª—è–º–∏
- ‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è AIResponse –∑ —É—Å—ñ–º–∞ –ø–æ–ª—è–º–∏
- ‚úÖ –í–∞–ª—ñ–¥–∞—Ü—ñ—è –æ–±–æ–≤'—è–∑–∫–æ–≤–∏—Ö –ø–æ–ª—ñ–≤
- ‚úÖ –°–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —É dict/JSON
- ‚úÖ –†–æ–±–æ—Ç–∞ –∑ raw –≤—ñ–¥–ø–æ–≤—ñ–¥—è–º–∏
- ‚úÖ –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è AIProvider –ø—Ä–æ—Ç–æ–∫–æ–ª—É

## üîÑ –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è

–ù–∞ –¥–∞–Ω–∏–π –º–æ–º–µ–Ω—Ç —Ü–µ –±–∞–∑–æ–≤—ñ –∞–±—Å—Ç—Ä–∞–∫—Ü—ñ—ó. –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:

1. **–°—Ç–≤–æ—Ä–∏—Ç–∏ OpenAI Provider** ‚Äî —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ —ñ—Å–Ω—É—é—á–æ—ó —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó
2. **–°—Ç–≤–æ—Ä–∏—Ç–∏ Gemini Provider** ‚Äî –Ω–æ–≤–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è
3. **–°—Ç–≤–æ—Ä–∏—Ç–∏ AI Router** ‚Äî –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—è –º—ñ–∂ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
4. **–û–Ω–æ–≤–∏—Ç–∏ Agents** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–æ–≤–æ–≥–æ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É

## üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è

–î–µ—Ç–∞–ª—å–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏: [docs/MULTI_AI_ARCHITECTURE.md](../../docs/MULTI_AI_ARCHITECTURE.md)

## ‚úÖ –°—Ç–∞—Ç—É—Å

- [x] AIResponse –º–æ–¥–µ–ª—å —Å—Ç–≤–æ—Ä–µ–Ω–æ
- [x] AIProvider –ø—Ä–æ—Ç–æ–∫–æ–ª –≤–∏–∑–Ω–∞—á–µ–Ω–æ
- [x] –¢–µ—Å—Ç–∏ –Ω–∞–ø–∏—Å–∞–Ω–æ (10/10 passing)
- [x] **OpenAI Provider —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ (16/16 tests passing)**
- [x] **Gemini Provider —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ (24/24 tests passing)**
- [ ] AI Router (–Ω–∞—Å—Ç—É–ø–Ω–∏–π –∫—Ä–æ–∫)

**–ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:** ‚úÖ 50/50 tests passing

---

## üîå Google Gemini Provider

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

```python
from src.core.ai import GeminiClient, AIResponse

# Initialize client
client = GeminiClient(
    api_key="AIza...",  # Optional, reads from GEMINI_API_KEY or GOOGLE_API_KEY
    model_default="gemini-2.0-flash-exp"
)

# Generate text
response: AIResponse = await client.generate(
    prompt="Explain quantum computing in simple terms",
    temperature=0.7,
    max_tokens=500
)

print(response.text)  # Generated text
print(response.total_tokens)  # Token usage

# Count tokens
token_count = await client.count_tokens("Some text to count")
print(f"Tokens: {token_count}")
```

### –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ

- ‚úÖ **REST API based** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Google Generative AI REST API v1beta
- ‚úÖ **Rate limit handling** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—ñ retry –∑ exponential backoff
- ‚úÖ **Token counting** ‚Äî –≤–±—É–¥–æ–≤–∞–Ω–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É —Ç–æ–∫–µ–Ω—ñ–≤
- ‚úÖ **Token tracking** ‚Äî –ø–æ–≤–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤
- ‚úÖ **Error handling** ‚Äî –¥–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫
- ‚úÖ **Flexible configuration** ‚Äî –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –≤—Å—ñ—Ö Gemini –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- ‚úÖ **Protocol compliant** ‚Äî —Ä–µ–∞–ª—ñ–∑—É—î AIProvider —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- ‚úÖ **Context manager support** ‚Äî async with –ø—ñ–¥—Ç—Ä–∏–º–∫–∞

### –ü—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ

```python
# Gemini 2.0 (experimental)
client = GeminiClient(model_default="gemini-2.0-flash-exp")

# Gemini 1.5 (stable)
client = GeminiClient(model_default="gemini-1.5-pro")
client = GeminiClient(model_default="gemini-1.5-flash")

# Gemini 1.0
client = GeminiClient(model_default="gemini-1.0-pro")
```

### –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è

```bash
# .env file
GEMINI_API_KEY=AIza...  # Preferred
# OR
GOOGLE_API_KEY=AIza...  # Alternative
```

### Generation Parameters

```python
response = await client.generate(
    prompt="Your prompt",
    temperature=0.7,       # Creativity (0.0 - 2.0)
    max_tokens=1000,       # Maximum output tokens
    top_p=0.95,           # Nucleus sampling
    top_k=40              # Top-k sampling
)
```

### –¢–µ—Å—Ç–∏

```bash
pytest tests/core/ai/test_gemini_client.py -v
```

**Test coverage:**
- ‚úÖ Initialization (6 tests)
  - With/without API key
  - Multiple env var names
  - Custom models and timeouts
- ‚úÖ Text generation (9 tests)
  - Success cases
  - Custom models and parameters
  - Multiple text parts handling
  - Rate limit retry
  - Error handling
- ‚úÖ Token counting (4 tests)
- ‚úÖ Embeddings (1 test - placeholder)
- ‚úÖ Protocol compliance (4 tests)

---

## üîå OpenAI Provider

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

```python
from src.core.ai import OpenAIClient, AIResponse

# Initialize client
client = OpenAIClient(
    api_key="sk-...",  # Optional, reads from OPENAI_API_KEY env var
    model_default="gpt-4o-mini"
)

# Generate text
response: AIResponse = await client.generate(
    prompt="Explain quantum computing",
    temperature=0.7,
    max_tokens=500
)

print(response.text)  # Generated text
print(response.total_tokens)  # Token usage
```

### –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ

- ‚úÖ **Rate limit handling** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—ñ retry –∑ exponential backoff
- ‚úÖ **Token tracking** ‚Äî –ø–æ–≤–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤
- ‚úÖ **Error handling** ‚Äî –¥–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫
- ‚úÖ **Flexible configuration** ‚Äî –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –≤—Å—ñ—Ö OpenAI –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- ‚úÖ **Protocol compliant** ‚Äî —Ä–µ–∞–ª—ñ–∑—É—î AIProvider —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å

### –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è

```bash
# .env file
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini  # Default model
```

### –¢–µ—Å—Ç–∏

```bash
pytest tests/core/ai/test_openai_client.py -v
```

**Test coverage:**
- ‚úÖ Initialization (with/without API key)
- ‚úÖ Text generation (success, retries, errors)
- ‚úÖ Rate limit handling
- ‚úÖ Token counting (not implemented placeholder)
- ‚úÖ Embeddings (not implemented placeholder)
- ‚úÖ Protocol compliance

---
